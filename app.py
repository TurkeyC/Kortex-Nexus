import streamlit as st
import os
import torch
import time
from modules.ui import setup_page, apply_custom_styles, display_header, render_sidebar, render_chat_area, \
    display_assistant_response
from modules.models import get_model
from modules.retrieval import KnowledgeBase
import config

# å¯¼å…¥äºŒé˜¶æ®µå¼€å‘æ¨¡å—
from modules.retrieval_engines import KeywordRetriever, HybridRetriever, TreeStructureRetriever
from modules.rag import RAGPipeline
from modules.kb_manager import KnowledgeBaseManager
from modules.conversation_manager import ConversationTree, ContextManager
# ä¿®æ­£å¯¼å…¥, ä½¿ç”¨æ­£ç¡®çš„ui_components
from modules.ui_components import render_js_tree_selector, render_search_results


# é¢„å¼€å‘åŠŸèƒ½å¯¼å…¥ï¼ˆé¢„ç•™ï¼‰
from modules.advanced.swarms import Swarms
from modules.advanced.storm import Storm
from modules.advanced.consciousness import ConsciousnessFlow
from modules.advanced.hook import Hook
from modules.advanced.long_memory import LongMemory


import warnings
import logging
# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.ERROR)


# ç¬¬ä¸€ä¸ªStreamlitå‘½ä»¤å¿…é¡»æ˜¯set_page_config
def main():
    # 1. è®¾ç½®é¡µé¢é…ç½® - å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªStreamlitå‘½ä»¤
    setup_page(
        title=config.UI_CONFIG["title"],
        description=config.UI_CONFIG["description"],
        theme=config.UI_CONFIG["theme"]
    )

    # 2. åº”ç”¨è‡ªå®šä¹‰æ ·å¼
    apply_custom_styles(theme=config.UI_CONFIG["theme"])

    # 3. æ˜¾ç¤ºæ ‡é¢˜å’Œæè¿°
    display_header(
        title=config.UI_CONFIG["title"],
        description=config.UI_CONFIG["description"]
    )

    # 4. æ£€æµ‹GPUæ”¯æŒ
    if torch.cuda.is_available() and config.ENABLE_GPU:
        st.sidebar.success(f"å·²æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
        device_info = f"CUDA: {torch.version.cuda} | æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB"
        st.sidebar.info(device_info)
    else:
        st.sidebar.warning("æœªæ£€æµ‹åˆ°GPUæˆ–GPUæ”¯æŒæœªå¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼è¿è¡Œ")
        config.ENABLE_GPU = False
        config.EMBEDDING_DEVICE = "cpu"

    # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§å¹¶æä¾›å‹å¥½æç¤º
    is_local_model_available = False
    is_api_model_available = False

    try:
        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æœåŠ¡å¯ç”¨æ€§
        import requests
        ollama_available = False
        lmstudio_available = False

        # å°è¯•è¿æ¥Ollama
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                ollama_available = True
                st.sidebar.success("âœ… OllamaæœåŠ¡å·²è¿æ¥")
        except requests.RequestException:
            pass
        # # æ˜¾ç¤ºå®‰è£…æŒ‡å—é“¾æ¥
        # st.sidebar.markdown("""
        # [å®‰è£…Ollama](https://ollama.com/download)
        # ```bash
        # # å®‰è£…åè¿è¡Œ:
        # ollama pull llama3
        # ```
        # """)

        # å°è¯•è¿æ¥LM Studio (é€šå¸¸åœ¨1234æˆ–8000ç«¯å£)
        try:
            response = requests.get("http://localhost:1234/v1/models", timeout=2)
            if response.status_code == 200:
                lmstudio_available = True
                st.sidebar.success("âœ… LM StudioæœåŠ¡å·²è¿æ¥")
        except requests.RequestException:
            pass

        is_local_model_available = ollama_available or lmstudio_available

        if not is_local_model_available:
            st.sidebar.warning("âš ï¸ æœªæ£€æµ‹åˆ°Ollamaæˆ–LM StudioæœåŠ¡")
    except Exception as e:
        st.sidebar.warning(f"âš ï¸ æ£€æµ‹æœ¬åœ°æ¨¡å‹æœåŠ¡æ—¶å‡ºé”™: {str(e)}")


    # æ£€æŸ¥APIå¯†é’¥v1
    # æ£€æŸ¥å„ç§å¯èƒ½çš„APIå¯†é’¥
    api_keys = {
        "OpenAI": os.environ.get("OPENAI_API_KEY") or
                  (hasattr(st.session_state, 'openai_api_key') and st.session_state.openai_api_key),
        "Moonshot": os.environ.get("MOONSHOT_API_KEY") or
                    (hasattr(st.session_state, 'moonshot_api_key') and st.session_state.moonshot_api_key),
        "Azure": os.environ.get("AZURE_OPENAI_API_KEY") or
                 (hasattr(st.session_state, 'azure_api_key') and st.session_state.azure_api_key),
        "Anthropic": os.environ.get("ANTHROPIC_API_KEY") or
                     (hasattr(st.session_state, 'anthropic_api_key') and st.session_state.anthropic_api_key)
    }

    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•APIå¯†é’¥å¯ç”¨
    available_apis = [name for name, key in api_keys.items() if key]
    if available_apis:
        is_api_model_available = True
        st.sidebar.success(f"âœ… å·²é…ç½®APIå¯†é’¥: {', '.join(available_apis)}")
    else:
        st.sidebar.info("â„¹ï¸ æš‚æœªæ£€æµ‹åˆ°é…ç½®ä»»ä½•å¯ç”¨APIå¯†é’¥ï¼Œéœ€ä½¿ç”¨æœ¬åœ°æ¨¡å‹")

    if not is_local_model_available and not is_api_model_available:
        st.warning("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°å¯ç”¨çš„æ¨¡å‹æœåŠ¡ã€‚è¯·åœ¨ä¾§è¾¹æ é…ç½®APIå¯†é’¥æˆ–å¯åŠ¨æœ¬åœ°æ¨¡å‹æœåŠ¡ã€‚")

    # é‡è¦ï¼šå…ˆåˆå§‹åŒ–æ¨¡å‹ï¼Œå†åˆå§‹åŒ–ä¾èµ–æ¨¡å‹çš„ç»„ä»¶
    # å¦‚æœæ¨¡å‹å°šæœªåˆå§‹åŒ–ï¼Œåˆ™åˆå§‹åŒ–
    if "model" not in st.session_state:
        st.session_state.model = get_model(config.DEFAULT_MODEL)
        
    # å®šä¹‰æ¨¡å‹æ›´æ”¹å›è°ƒv1
    def on_model_change(model_name):
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»æ›´æ”¹
        current_model_type = getattr(st.session_state.get("model", None), "__model_type__", "")

        if model_name != current_model_type:
            st.session_state.model = get_model(model_name)
            
    # åˆå§‹åŒ–çŸ¥è¯†åº“v1
    if "knowledge_base" not in st.session_state:
        st.session_state.knowledge_base = KnowledgeBase()
        # å°è¯•åŠ è½½ç°æœ‰å‘é‡åº“
        if st.session_state.knowledge_base.load_existing_vectorstore():
            st.success("æˆåŠŸåŠ è½½ç°æœ‰çŸ¥è¯†åº“")
            # å­˜å‚¨å‘é‡ç»´åº¦
            if st.session_state.knowledge_base.vector_store:
                st.session_state.vector_dimension = st.session_state.knowledge_base.vector_dimension
        else:
            st.warning("æ— æ³•åŠ è½½çŸ¥è¯†åº“ï¼Œè¯·ä¸Šä¼ æ–‡æ¡£æˆ–é‡å»ºç´¢å¼•")
    
    # çŸ¥è¯†åº“é‡å»ºæŒ‰é’® - æ”¾åœ¨æ›´çªå‡ºçš„ä½ç½®
    if hasattr(st.session_state, "knowledge_base") and "vector_dimension" in st.session_state:
        kb_col1, kb_col2 = st.columns([3, 1])
        with kb_col1:
            st.info(f"çŸ¥è¯†åº“çŠ¶æ€: {'å·²åŠ è½½' if st.session_state.knowledge_base.vector_store else 'æœªåŠ è½½'}")
        with kb_col2:
            if st.button("é‡æ–°æ„å»ºçŸ¥è¯†åº“ç´¢å¼•", key="rebuild_index"):
                with st.spinner("æ­£åœ¨é‡æ–°æ„å»ºçŸ¥è¯†åº“ç´¢å¼•..."):
                    # æ¸…é™¤ç°æœ‰ç´¢å¼•
                    st.session_state.knowledge_base.vector_store = None
                    # é‡æ–°åŠ è½½çŸ¥è¯†åº“
                    st.session_state.knowledge_base.load_knowledge_base()
                st.success("çŸ¥è¯†åº“ç´¢å¼•å·²é‡å»º")
                # é‡æ–°åŠ è½½é¡µé¢
                st.rerun()

    # åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨v2
    if "kb_manager" not in st.session_state:
        st.session_state.kb_manager = KnowledgeBaseManager(config.KNOWLEDGE_BASE_DIR)

    # åˆå§‹åŒ–å¯¹è¯æ ‘v2
    if "conversation_tree" not in st.session_state:
        st.session_state.conversation_tree = ConversationTree()
        st.session_state.conversation_tree.start_conversation(
            "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ä¸°å¯Œçš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"
        )

    # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨v2 - ç°åœ¨å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨model
    if "context_manager" not in st.session_state:
        st.session_state.context_manager = ContextManager(
            max_tokens=4000,
            llm=st.session_state.model
        )

    # åˆå§‹åŒ–å…³é”®è¯æ£€ç´¢å™¨v2 - åŒæ ·ä¾èµ–model
    if "keyword_retriever" not in st.session_state and hasattr(st.session_state, "knowledge_base"):
        # åŠ è½½æ–‡æ¡£æ•°æ®
        documents = []
        if st.session_state.knowledge_base.vector_store:
            for doc in st.session_state.knowledge_base.vector_store.docstore._dict.values():
                documents.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata
                })

            # åˆ›å»ºå…³é”®è¯æ£€ç´¢å™¨
            st.session_state.keyword_retriever = KeywordRetriever()
            st.session_state.keyword_retriever.add_documents(documents)
            
            # åˆ›å»ºæ ‘çŠ¶ç»“æ„æ£€ç´¢å™¨
            st.session_state.tree_retriever = TreeStructureRetriever(st.session_state.knowledge_base)
            
            # åˆå§‹åŒ–é€‰ä¸­çš„æ ‘èŠ‚ç‚¹
            if "selected_tree_nodes" not in st.session_state:
                st.session_state.selected_tree_nodes = []

            # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
            st.session_state.hybrid_retriever = HybridRetriever(
                vector_retriever=st.session_state.knowledge_base,
                keyword_retriever=st.session_state.keyword_retriever,
                tree_retriever=st.session_state.tree_retriever,
                weights=config.RETRIEVAL_CONFIG["weights"]
            )

            # åˆ›å»ºRAGç®¡é“
            st.session_state.rag_pipeline = RAGPipeline(
                llm=st.session_state.model,
                retriever=st.session_state.hybrid_retriever
            )

    # åˆå§‹åŒ–é¢„å¼€å‘åŠŸèƒ½ï¼ˆé¢„ç•™ï¼‰
    if "advanced_features" not in st.session_state:
        st.session_state.advanced_features = {
            "swarms": Swarms(config.ADVANCED_CONFIG["swarms"]),
            "storm": Storm(config.ADVANCED_CONFIG["storm"]),
            "consciousness": ConsciousnessFlow(config.ADVANCED_CONFIG["consciousness"]),
            "hook": Hook(config.ADVANCED_CONFIG["hook"]),
            "long_memory": LongMemory(config.ADVANCED_CONFIG["long_memory"])
        }

        # åˆå§‹åŒ–é€‰æ‹©çš„åŠŸèƒ½æ¨¡å¼
        st.session_state.active_feature = "standard"  # æ ‡å‡†æ¨¡å¼

    # å®šä¹‰åŠŸèƒ½æ¨¡å¼æ›´æ”¹å›è°ƒ
    def on_feature_mode_change(feature_mode):
        st.session_state.active_feature = feature_mode

    # æ¸²æŸ“ä¾§è¾¹æ 
    sidebar_config = render_sidebar(
        available_models=list(config.MODEL_CONFIG.keys()),
        on_model_change=on_model_change,
        on_feature_mode_change=on_feature_mode_change,
        available_features=["standard", "swarms", "storm", "consciousness", "hook"],
        advanced_features_enabled=config.UI_CONFIG["advanced_features"]
    )

    # æ›´æ–°æ¨¡å‹é…ç½®
    st.session_state.model.set_system_prompt(sidebar_config["system_prompt"])
    st.session_state.model.set_temperature(sidebar_config["temperature"])

    # æ¸²æŸ“æ ‘çŠ¶ç»“æ„é€‰æ‹©å™¨ - ä½¿ç”¨ä¿®æ”¹åçš„ç»„ä»¶
    if "tree_retriever" in st.session_state:
        with st.expander("ğŸ“š çŸ¥è¯†åº“ç›®å½•", expanded=False):
            # è·å–æ ‘ç»“æ„
            tree = st.session_state.tree_retriever.get_tree()

            # å®šä¹‰èŠ‚ç‚¹é€‰æ‹©å›è°ƒ
            def on_node_select(selected_nodes):
                st.session_state.selected_tree_nodes = selected_nodes

            # ä½¿ç”¨æ‰å¹³åŒ–ç»„ä»¶æ¸²æŸ“
            render_js_tree_selector(
                tree,
                st.session_state.selected_tree_nodes,
                on_node_select
            )

    # å®šä¹‰æ¶ˆæ¯å‘é€å›è°ƒv1
    def on_send(user_input):
        with st.spinner("æ€è€ƒä¸­..."):
            # è·å–èŠå¤©å†å²
            chat_history = [
                {"role": msg["role"], "content": msg["content"]}
                for msg in st.session_state.messages[:-1]  # ä¸åŒ…æ‹¬æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
            ]

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯æ ‘
            st.session_state.conversation_tree.add_message({"role": "user", "content": user_input})

            # è·å–å¯¹è¯è·¯å¾„
            chat_history = st.session_state.conversation_tree.get_conversation_path()

            # å‹ç¼©å†å²å¯¹è¯
            compressed_history = st.session_state.context_manager.compress_history(chat_history)

            # ä½¿ç”¨RAGç®¡é“å¤„ç†æŸ¥è¯¢
            if hasattr(st.session_state, "rag_pipeline"):
                try:
                    # å°†é€‰æ‹©çš„æ ‘èŠ‚ç‚¹ä¼ é€’ç»™æ£€ç´¢å™¨
                    selected_nodes = st.session_state.selected_tree_nodes if hasattr(st.session_state, "selected_tree_nodes") else []
                    
                    rag_result = st.session_state.rag_pipeline.process(
                        user_input, 
                        compressed_history,
                        selected_nodes=selected_nodes  # ä¼ é€’é€‰ä¸­çš„èŠ‚ç‚¹
                    )
                    response = rag_result["response"]

                    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆè°ƒè¯•ç”¨ï¼‰
                    with st.expander("æŸ¥çœ‹æ£€ç´¢ä¸Šä¸‹æ–‡", expanded=False):
                        st.write("é‡å†™æŸ¥è¯¢ï¼š", rag_result["rewritten_query"])
                        st.write("æ£€ç´¢ç»“æœï¼š")
                        for i, result in enumerate(rag_result["results"]):
                            st.write(f"[{i+1}] ç›¸å…³åº¦: {result['score']:.4f}")
                            st.write(f"æ¥æº: {result['metadata']['source']}")
                            st.write(result["content"][:200] + "...")
                            st.write("---")
                except Exception as e:
                    # å¦‚æœRAGç®¡é“å¤±è´¥ï¼Œå›é€€åˆ°åŸºæœ¬æ¨¡å‹å›ç­”
                    st.error(f"RAGå¤„ç†å¤±è´¥: {str(e)}")
                    response = st.session_state.model.generate_response(user_input, compressed_history)
            else:
                # å›é€€åˆ°åŸºæœ¬æ¨¡å‹å›ç­”
                response = st.session_state.model.generate_response(user_input, compressed_history)

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å¯¹è¯æ ‘
            st.session_state.conversation_tree.add_message({"role": "assistant", "content": response})

            # æ˜¾ç¤ºå›å¤
            display_assistant_response(response)

            # é•¿ä¸Šä¸‹æ–‡è®°å¿†å¤„ç† (é¢„ç•™)
            if config.ADVANCED_CONFIG["long_memory"]["enabled"]:
                # æ·»åŠ æ¶ˆæ¯åˆ°é•¿è®°å¿†ç³»ç»Ÿ
                st.session_state.advanced_features["long_memory"].add_message({
                    "role": "user",
                    "content": user_input
                })
                st.session_state.advanced_features["long_memory"].add_message({
                    "role": "assistant",
                    "content": response
                })

    # æ¸²æŸ“èŠå¤©åŒºåŸŸ
    render_chat_area(on_send=on_send)


if __name__ == "__main__":
    main()
